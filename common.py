#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 14:15:25 2020

@author: hallavar
"""

import glob
import keras
import itertools
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

def load_firstlayer_data(path, data):
    df = pd.read_csv(path+'StaticLayer_Intent_and_Permission_Bening&malware_'+data+'Smaples.csv')

    d ={'Malware': True, 'Benign': False}
    df['Binary_Type']=df['Binary_Type'].map(d)
      
    labels=df['Binary_Type']
    df.drop(['Binary_Type','<family>','<category>', '<MD5>'], inplace=True, axis=1)
    samples=df
    return samples, labels

def load_secondlayer_data(path, hsh, static_df, static_model):
    df=pd.read_csv(path)
    df[' Label']=df[' Label'].map({'BENIGN':0})
    df[' Label']=df[' Label'].fillna(1)
    y=df[' Label'].iloc[0]
    x=df.drop(columns=[' Label'], axis=1)
    u=static_df.loc[static_df['<MD5>']==hsh]
    u=u.drop(['Binary_Type','<family>','<category>', '<MD5>'], axis=1)
    pred=static_model.predict(u)
    x['Static prediction']=pred[0]
    return x, y

def create_secondlayer_partition(Fstlayer_data_path, Sndlayer_data_path, SndLayer_Training_ratio, Global_Testing_Ratio): 
    df_global=pd.read_csv(Fstlayer_data_path+'/StaticLayer_Intent_and_Permission_Bening&malware_TrainingSmaples.csv')
    hashs=np.asarray(df_global['<MD5>'])
    hashs=train_test_split(hashs, test_size=Global_Testing_Ratio, shuffle=True)
    hashs[0]=train_test_split(hashs[0], train_size=SndLayer_Training_ratio, shuffle=True)
    path=[[change_hash_into_path(hashs[0][0], Sndlayer_data_path),change_hash_into_path(hashs[0][1], Sndlayer_data_path)],change_hash_into_path(hashs[1], Sndlayer_data_path)]
    partition={'train':[path[0][0],hashs[0][0]],'validation':[path[0][1],hashs[0][1]]}
    return partition, np.asarray(hashs[1])

def preprocess_SndLayer_Dataset(df, prepro='preprocess.csv'):
    prepro=pd.read_csv(prepro).to_dict('index')[0]
    df=df.astype('string')
    df=df.astype(prepro)
    df=df.drop(columns=[' Timestamp'])
    return df

def create_image_from_a_DataFrame(df, n):
    uniques=df.nunique()
    values=[]
    encodes=[]
    for column in df.columns:
        if df[column].dtype != np.float64:
            values.append({column:uniques[column]})
        else:
            values.append(column)
    for dic in values:
        if len(dic) == 1:
            column=list(dic.keys())[0]
            encode={}
            for i, value in enumerate(df[column].unique()):
                encode[value] = n/df[column].nunique()*i
        else:
            column=dic
            sorte=df[column].sort_values()
            temp = sorte*n/df[column].max()
            temp = temp.to_numpy()
            encode=dict(zip(sorte.to_numpy(),temp))
        encodes.append(encode)
    encoder=dict(zip(df.columns, encodes))
    df = df.replace(encoder)
    df=df.to_numpy(dtype=np.uint8)
    return df
    
def change_hash_into_path(hashs, path):
    paths=[]
    list_csv=glob.glob(path+'/***-CSVs/**/*.csv', recursive=True)
    for hsh in hashs:
        for path in list_csv:
            if hsh in path:
                paths.append(path)
    return paths


def get_metrics(true_labels,predictions, get_cm=True):
    cm=confusion_matrix(true_labels,predictions)

    tn, fp, fn, tp = cm.ravel()
    N=len(predictions)

    accuracy=(tn+tp)/N
    precision=tp/(tp+fp)
    recall=tp/(tp+fn)
    f1=2*(precision*recall)/(precision+recall)
    
    if get_cm:
        return cm, accuracy, precision, recall, f1
    else:
        return accuracy, precision, recall, f1
    
class DataGenerator(keras.utils.Sequence):
    
    def __init__(self, list_IDs_hash, static_model, static_df, canals=256, batch_size=32, n_classes=2, shuffle=True):
        self.list_IDs = list_IDs_hash[0]
        self.batch_size = batch_size
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.hsh = list_IDs_hash[1]
        self.static_model = static_model
        self.static_df = static_df
        self.canals = canals
        self.on_epoch_end()

    def _len_(self):
        return int(np.floor(len(self.list_IDs) / self.batch_size))
    
    def __getitem__(self, index):
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        X, y = self.__data_generation(list_IDs_temp)
        return X, y
    
    def on_epoch_end(self):
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        X = []
        Y = []
        for i, ID in enumerate(list_IDs_temp):
            x, y = load_secondlayer_data(ID, self.hsh[i], self.static_df, self.static_model)
            if self.canals != None :
                X.append(create_image_from_a_DataFrame(x,self.canals))
            else:
                X.append(x)
            Y.append(y)
        return np.asarray(X, dtype=object), np.asarray(Y, dtype=int)
            
            
