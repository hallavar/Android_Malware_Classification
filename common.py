#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 14:15:25 2020

@author: hallavar
"""

import glob
import tensorflow.keras as keras
# import itertools
import ipaddress
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

def load_firstlayer_data(path, data):
    df = pd.read_csv(path+'StaticLayer_Intent_and_Permission_Bening&malware_'+data+'Smaples.csv')

    d ={'Malware': True, 'Benign': False}
    df['Binary_Type']=df['Binary_Type'].map(d)
      
    df=df.drop_duplicates(['<MD5>'])
    
    labels=df['Binary_Type']
    
    df.drop(['Binary_Type','<family>','<category>', '<MD5>'], inplace=True, axis=1)
    samples=df
    return samples, labels

def load_secondlayer_data(path, hsh, static_df):
    df=pd.read_csv(path)
    df=preprocess_SndLayer_Dataset(df)
    y=df[' Label'].iloc[0]
    tmp=np.zeros((5,))
    tmp[int(y)]=1
    y=tmp
    x=df.drop(columns=[' Label'], axis=1)
    u=static_df[static_df['<MD5>']==hsh]
    # u=u.drop(['Binary_Type','<family>','<category>', '<MD5>'], axis=1)
    # pred=static_model.predict(u)
    # x['Static prediction']=pred[0]
    x['Static prediction']=u['Static prediction'].values[0]
    #x['Static prediction']=x['Static prediction'].map({True: 1, False: 0})
    return x, y#.to_numpy()

def create_secondlayer_partition(Fstlayer_data_path, Sndlayer_data_path, SndLayer_Training_ratio, Global_Testing_Ratio): 
    df_global=pd.read_csv(Fstlayer_data_path+'/StaticLayer_Intent_and_Permission_Bening&malware_TrainingSmaples.csv')
    hashs=np.asarray(df_global['<MD5>'])
    hashs=train_test_split(hashs, test_size=Global_Testing_Ratio, shuffle=True)
    hashs[0]=train_test_split(hashs[0], train_size=SndLayer_Training_ratio, shuffle=True)
    path=[[change_hash_into_path(hashs[0][0], Sndlayer_data_path),change_hash_into_path(hashs[0][1], Sndlayer_data_path)],change_hash_into_path(hashs[1], Sndlayer_data_path)]
    partition={'train':[path[0][0],hashs[0][0]],'validation':[path[0][1],hashs[0][1]]}
    return partition, np.asarray(hashs[1])

def preprocess_SndLayer_Dataset(df):
    df[' Label']=df[' Label'].str.split('_',expand=True)[0]
    df[' Label']=df[' Label'].map({'BENIGN':0, 'SCAREWARE':1, 'SMSMALWARE':2,'RANSOMWARE':3, 'ADWARE':4})
    #df[' Label']=df[' Label'].map({'BENIGN':0, 'ADWARE':1, 'SCAREWARE':2, 'RANSOMWARE':3, 'SMSMALWARE':4, 'MALWARE':5})
    #df[' Label']=df[' Label'].fillna(1)
    df=df.drop(columns=[' Timestamp', 'Flow ID'])
    df=df.dropna()
    def transform_adress(li):
        u=[]
        for ip in li:
            try:
                u.append(int(ipaddress.ip_address(ip)))
            except ValueError:
                u.append(np.nan)
        return(u)
    df[' Source IP']= transform_adress(df[' Source IP'])
    df[' Destination IP']= transform_adress(df[' Destination IP'])
    df=df.astype('float32')
    df=df.dropna(subset=[' Source IP',' Destination IP'])
    df=df[df[' Flow Packets/s']!=np.inf]
    return df

def contain_string(li, string):
    for i in li:
        if string in i:
            return i
        
def Binary_Classification(arr):
    a=np.zeros(np.shape(arr))
    u=[]
    for i, pred in enumerate(arr):
        a[i][np.argmax(pred)]=1
        u.append([a[i][0],np.max(a[i][1:])])
    u=np.asarray(u)
    return u.argmax(axis=1)

# def create_image_from_a_DataFrame(df, n):
#     uniques=df.nunique()
#     values=[]
#     encodes=[]
#     for column in df.columns:
#         if df[column].dtype != np.float64:
#             values.append({column:uniques[column]})
#         else:
#             values.append(column)
#     for dic in values:
#         if len(dic) == 1:
#             column=list(dic.keys())[0]
#             encode={}
#             for i, value in enumerate(df[column].unique()):
#                 encode[value] = n/df[column].nunique()*i
#         else:
#             column=dic
#             sorte=df[column].sort_values()
#             temp = sorte*n/df[column].max()
#             temp = temp.to_numpy()
#             encode=dict(zip(sorte.to_numpy(),temp))
#         encodes.append(encode)
#     encoder=dict(zip(df.columns, encodes))
#     # encoder['Static prediction']={True: 1, False: 0}
#     df = df.replace(encoder)
#     df=df.to_numpy(dtype=np.uint8)
#     return df
    
def create_image_from_a_DataFrame(df, max_values, standardize=False):
    for column in df.drop(['Static prediction'], axis=1).columns:
        if max_values['maxi'][column]==max_values['mini'][column]:
            df=df.drop([column], axis=1)
        else:
            x=df[column]
            mean=max_values['mean'][column]/(max_values['maxi'][column]-max_values['mini'][column])
            std=max_values['std'][column]/(max_values['maxi'][column]-max_values['mini'][column])
            x=(x-max_values['mini'][column])/(max_values['maxi'][column]-max_values['mini'][column])
            if standardize:
                x=(x-mean)/std
            df[column]=x
    return df.to_numpy()

def get_stats_values(path):
    list_csv=glob.glob(path+'/CSVS/**/*.csv', recursive=True)  #Sur le PC fixe
    df=pd.DataFrame()
    for csv in list_csv:
        print(csv)
        tmp=pd.read_csv(csv)
        tmp=preprocess_SndLayer_Dataset(tmp)
        df=df.append(tmp)
    result=pd.DataFrame()
    result['mini']=df.min()
    result['maxi']=df.max()
    result['mean']=df.mean()
    result['std']=df.std()
    return result.to_dict() 

def change_hash_into_path(hashs, path):
    paths=[]
    #list_csv=glob.glob(path+'/***-CSVs/**/*.csv', recursive=True)  #Sur le PC portable
    list_csv=glob.glob(path+'/CSVS/**/*.csv', recursive=True)  #Sur le PC fixe
    for hsh in hashs:
        paths.append(contain_string(list_csv, hsh))
    return paths


def get_metrics(true_labels,predictions, get_cm=True):
    cm=confusion_matrix(true_labels,predictions)

    tn, fp, fn, tp = cm.ravel()
    N=len(predictions)

    accuracy=(tn+tp)/N
    precision=tp/(tp+fp)
    recall=tp/(tp+fn)
    f1=2*(precision*recall)/(precision+recall)
    
    if get_cm:
        return cm, accuracy, precision, recall, f1
    else:
        return accuracy, precision, recall, f1

def get_test_data(test, static_df, max_values, Snd_layer_data_path, padding=True):
    
    test=[change_hash_into_path(test, Snd_layer_data_path), test]
    test_generator = DataGenerator(test, static_df, max_values, batch_size=len(test[1]), shuffle=False, padding=padding)
    
    # test_samples=[]
    # test_labels=[]

    # for batch in test_generator:
    #     test_samples.extend(batch[0])
    #     test_labels.extend(batch[1])
    
    # return test_samples, np.asarray(test_labels)
    return test_generator[0][0], test_generator[0][1]
        
        
def data_generation(IDs, hashs, static_df, max_values, padding=-1):
    X = []
    Y = []
    for i, ID in enumerate(IDs):
        x, y = load_secondlayer_data(ID, hashs[i], static_df)
        arr = create_image_from_a_DataFrame(x,max_values)
        # if padding != None :
            # if i == 0:
            #     top = len(arr)
            # else : 
            #     top = max(len(arr), top)
            # pad=np.ones((top-np.shape(arr)[0],np.shape(arr)[1]))*padding
            # arr=np.concatenate((arr,pad),axis=0)
            #y=np.concatenate(())
            # for pad in range(len(arr), top):
            #     arr = np.append(arr,[[padding]*np.shape(arr)[1]],axis=0)
        X.append(arr)
        Y.append(y)
    if padding != None:
        max_len = max([len(x) for x in X])
        for i,x in enumerate(X):
            pad=np.ones((max_len-np.shape(x)[0],np.shape(x)[1]))*padding
            X[i]=np.concatenate((x,pad),axis=0)
    else:
        max_len = min([len(x) for x in X])
        for i,x in enumerate(X):
            X[i]=x[:max_len,]
    return np.asarray(X), np.asarray(Y,dtype='float32')
    
class DataGenerator(keras.utils.Sequence):
    
    def __init__(self, list_IDs_hash, static_df, max_values, batch_size=5, n_classes=2, shuffle=True, padding= -1):
        self.list_IDs = list_IDs_hash[0]
        self.batch_size = batch_size
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.hsh = list_IDs_hash[1]
        self.static_df = static_df
        self.max_values = max_values
        self.padding = padding
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.list_IDs) / self.batch_size))
    
    def __getitem__(self, index):
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
        list_IDs_temp = [self.list_IDs[k] for k in indexes]
        list_hashs_temp = [self.hsh[k] for k in indexes]
        #X, y = self.__data_generation(list_IDs_temp)
        X, y = data_generation(list_IDs_temp, list_hashs_temp, self.static_df, self.max_values, self.padding)
        y=y.reshape(self.batch_size,5)
        return X.reshape(self.batch_size,*np.shape(X[0])), y#, self.get_weight2(y)
    
    def get_weight(self, y):
        w=np.zeros(len(y))
        unique, counts = np.unique(y, return_counts=True, axis=0)
        #counts = 1 - counts/self.batch_size
        counts = (1 / counts)*(self.batch_size)/2.0
        for idx, label in enumerate(unique):
            t=np.where(np.all(y==label,axis=1))[0]
            w[t]=counts[idx]
        return w
    def get_weight2(self, y):
        # w0=0.6625294118
        # w1=9.57657658
        # w2=9.75229358
        # w3=10.2211538
        # w4=10.5247525
        # w5=1063
        w0=0.67569546
        w1=7.2109375
        w2=7.2109375
        w3=8.24107143
        w4=8.24107143
        weights=np.asarray([w0, w1, w2, w3, w4])#, w5])
        return np.dot(y, weights)
    
    def on_epoch_end(self):
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)
