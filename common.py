#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 14:15:25 2020

@author: hallavar
"""

import glob
import tensorflow.keras as keras
import ipaddress
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

def load_firstlayer_data(path, data): #Charge et prétraite les données statiques
    df = pd.read_csv(path+'StaticLayer_Intent_and_Permission_Bening&malware_'+data+'Smaples.csv')

    d ={'Malware': True, 'Benign': False}
    df['Binary_Type']=df['Binary_Type'].map(d)#Transformation de la valeur cible en format binaire
      
    df=df.drop_duplicates(['<MD5>'])#Suppresion des doublons
    
    labels=df['Binary_Type']#Extraction de la valeur cible
    
    df.drop(['Binary_Type','<family>','<category>', '<MD5>'], inplace=True, axis=1)
    samples=df#Création des observations
    return samples, labels

def load_secondlayer_data(path, hsh=None, static_df=None):#Chargement des données dynamique
    df=pd.read_csv(path)#Lecture du pcap de l'application
    df=preprocess_SndLayer_Dataset(df)#prétraitement du pcap
    y=df[' Label'].iloc[0]#Extraction de la valeur cible
    tmp=np.zeros((5,))
    tmp[int(y)]=1
    y=tmp#transformation d'une cible de dimension 1 en une cible de dimension 5
    x=df.drop(columns=[' Label'], axis=1)#Extractions des observations
    if static_df != None:
    	u=static_df[static_df['<MD5>']==hsh]#Chargement des données statiques relatives à l'application
    	x['Static prediction']=u['Static prediction'].values[0]#Création de la colonne 'Static prediction'
    return x, y

def create_secondlayer_partition(Fstlayer_data_path, Sndlayer_data_path, SndLayer_Training_ratio, Global_Testing_Ratio): #Répartitions des applications entre entrainements hybride, validation hybride et test global
    df_global=pd.read_csv(Fstlayer_data_path+'/StaticLayer_Intent_and_Permission_Bening&malware_TrainingSmaples.csv') #Chargement des données statiques restantes
    hashs=np.asarray(df_global['<MD5>'])#Récolte des hash d'applications restantes
    hashs=train_test_split(hashs, test_size=Global_Testing_Ratio, shuffle=True)#Séparation entre les données d'entrainement/validation pour le model hybride et les données réservé au test global
    hashs[0]=train_test_split(hashs[0], train_size=SndLayer_Training_ratio, shuffle=True)#Séparation entre les données d'entrainement et celles de validation
    path=[[change_hash_into_path(hashs[0][0], Sndlayer_data_path),change_hash_into_path(hashs[0][1], Sndlayer_data_path)],change_hash_into_path(hashs[1], Sndlayer_data_path)]#Conversion des nom des application (donnés sous forme de hash) en des chemins
    partition={'train':[path[0][0],hashs[0][0]],'validation':[path[0][1],hashs[0][1]]}#création de la partition
    return partition, np.asarray(hashs[1])

def preprocess_SndLayer_Dataset(df):#prétraitement des données dynamiques
    df[' Label']=df[' Label'].str.split('_',expand=True)[0]#prétraitement de la colonne de label
    df[' Label']=df[' Label'].map({'BENIGN':0, 'SCAREWARE':1, 'SMSMALWARE':2,'RANSOMWARE':3, 'ADWARE':4})#transformation du label en valeur numérique
    df=df.drop(columns=[' Timestamp', 'Flow ID'])#suppressions des colonnes inutiles
    df=df.dropna()#Suppression des lignes presentant des NA
    def transform_adress(li):#Cette fonction transforme les colonnes d'adresse IP au format X.X.X.X vers un format décimal
        u=[]
        for ip in li:
            try:#Il peut exister des addresses incompletes
                u.append(int(ipaddress.ip_address(ip)))
            except ValueError:
                u.append(np.nan)
        return(u)
    df[' Source IP']= transform_adress(df[' Source IP'])#transformations des colonnes d'adresse IP
    df[' Destination IP']= transform_adress(df[' Destination IP'])
    df=df.astype('float32')#Conversion des données entieres en float
    df=df.dropna(subset=[' Source IP',' Destination IP'])#Suppression des lignes où l'IP a été mal transformé
    df=df[df[' Flow Packets/s']!=np.inf]#Cette valeur peut parfois etr inf. Nous supprimons de telles valeur
    return df

def contain_string(li, string):#Fonction renvoyant la position dans une liste de string, d'une string donné en argumant exemple contain_string(['a','b','c','d'],'b') renvoie 1
    for i in li:
        if string in i:
            return i
        
def Binary_Classification(arr):#Transforme une classification multiclasse en classification binaire
    #a=np.zeros(np.shape(arr))
    u=[]
    for i, pred in enumerate(arr):#Pour chaque prédiction
        #a[i][np.argmax(pred)]=1
        u.append([pred[0],np.max(pred[1:])])#si pred=[1,0,0,0] alors u =[1,0] sinon u = [0,1]
    u=np.asarray(u)
    return u.argmax(axis=1)#retourn 1 si [0,1] ou 0 si [1,0]
    
def create_image_from_a_DataFrame(df, max_values, standardize=False):#Normalize un pcap d'application
    if max_values != None
    	for column in df.drop(['Static prediction'], axis=1).columns:#On ne considere pas la valeur cible
    		if max_values['maxi'][column]==max_values['mini'][column]:#Si la valeur ne change pas au cours du temp, on n'utilise pas cette valeur
            		df=df.drop([column], axis=1)
        	else:
            		x=df[column]
            		mean=max_values['mean'][column]/(max_values['maxi'][column]-max_values['mini'][column])#moyenne normalisée
            		std=max_values['std'][column]/(max_values['maxi'][column]-max_values['mini'][column])#déviation standart normalisée
            		x=(x-max_values['mini'][column])/(max_values['maxi'][column]-max_values['mini'][column])#On normalise toute les valeurs de la colonne
            		if standardize:#Si les variables suivent des lois gaussiennes
                		x=(x-mean)/std#On corrige par la moyenne et la déviation standart
            	df[column]=x
    	return df.to_numpy()#On renvoie une matrice
    else:
    	return df.to_numpy()

def get_stats_values(path):#Génére les valeurs statistiques pour chaque colonnes
    list_csv=glob.glob(path+'/CSVS/**/*.csv', recursive=True)  #On considere l'ensemble des applications
    df=pd.DataFrame()
    for csv in list_csv:#On chare l'ensemble des pcap de toutes les applications
        print(csv)
        tmp=pd.read_csv(csv)
        tmp=preprocess_SndLayer_Dataset(tmp)#On préprocess chaque PCAP
        df=df.append(tmp)#On a ici un dataframe contenant tous les flux réseaux du jeu de données (!!! Variable TRES LOURDE)
    result=pd.DataFrame()
    result['mini']=df.min()#On extrait les valeurs minimales pour chaques colonnes
    result['maxi']=df.max()#On extrait les valeurs maximales pour chaques colonnes
    result['mean']=df.mean()#On extrait les valeurs moyennes pour chaques colonnes
    result['std']=df.std()#On extrait la déviation standart pour chaque^ colonnes
    return result.to_dict() 

def change_hash_into_path(hashs, path):#Fonction qui change un hash d'application en un chemin vers son pcap
    paths=[]
    list_csv=glob.glob(path+'/CSVS/**/*.csv', recursive=True)  #liste tout les pcap du dataset
    for hsh in hashs:
        paths.append(contain_string(list_csv, hsh))#trouve le hash parmis la liste de fichier csv disponibles
    return paths


def get_metrics(true_labels,predictions, get_cm=True):#Calcule diffentes métrique d'evaluation d'un modele
    cm=confusion_matrix(true_labels,predictions)

    tn, fp, fn, tp = cm.ravel()
    N=len(predictions)

    accuracy=(tn+tp)/N
    precision=tp/(tp+fp)
    recall=tp/(tp+fn)
    f1=2*(precision*recall)/(precision+recall)
    
    if get_cm:
        return cm, accuracy, precision, recall, f1
    else:
        return accuracy, precision, recall, f1

def get_test_data(test, static_df, max_values, Snd_layer_data_path, padding=True):#Renvoie l'ensemble des donée de test prétraité
    
    test=[change_hash_into_path(test, Snd_layer_data_path), test]#crée un liste avec l'ensemble des chemins et l'ensemble des hash correspondants
    test_generator = DataGenerator(test, static_df, max_values, batch_size=len(test[1]), shuffle=False, padding=padding)#Creer un générateur de donnée d'un seul batch
    
    return test_generator[0][0], test_generator[0][1]#renvoie les observations et les labels correspondants
        
        
def data_generation(IDs, hashs=None, static_df=None, max_values=None, padding=-1):#Genere un batch de donné depuis une liste de chemin de pcap
    X = []
    Y = []
    for i, ID in enumerate(IDs):#pour chaque PCAP
        x, y = load_secondlayer_data(ID, hashs[i], static_df)#chargement des données du PCAP
        arr = create_image_from_a_DataFrame(x,max_values)#transformation des donnée du PCAP en matrice
        X.append(arr)#rajout de la matrice résultante aux observations
        Y.append(y)#rajout du label correspondant aux labels
    if padding != None:#Si on a indiqué une valeur de padding
        max_len = max([len(x) for x in X])#On considere la matrice la plus longue qu'on a généré
        for i,x in enumerate(X):#Pour chaque matrice
            pad=np.ones((max_len-np.shape(x)[0],np.shape(x)[1]))*padding#On crée une matrice de longuer longueur_max-longueur de la matrice rempli de la valeur de paddin
            X[i]=np.concatenate((x,pad),axis=0)#On concatene la matrice créé avec celle de l'observation
    else:#Si on ne veut pas de padding
        max_len = min([len(x) for x in X])#On considere la matrice la moins longue qu'on a généré
        for i,x in enumerate(X):#pour chaque matrice
            X[i]=x[:max_len,]#On ne conserve que les max_len premieres lignes
    return np.asarray(X), np.asarray(Y,dtype='float32')#On retourne le batch
    
class DataGenerator(keras.utils.Sequence):#Création d'un DataGenerator
    
    def __init__(self, list_IDs_hash, static_df, max_values=None, batch_size=5, n_classes=2, shuffle=True, padding= -1):#initialisation des parametre et definition des valeurs standart
        self.list_IDs = list_IDs_hash[0]
        self.batch_size = batch_size
        self.n_classes = n_classes
        self.shuffle = shuffle
        self.hsh = list_IDs_hash[1]
        self.static_df = static_df
        self.max_values = max_values
        self.padding = padding
        self.on_epoch_end()

    def __len__(self):
        return int(np.floor(len(self.list_IDs) / self.batch_size))
    
    def __getitem__(self, index):#récupere un batch de donnée
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]#liste d'index du batch
        list_IDs_temp = [self.list_IDs[k] for k in indexes]#list de PCAP du batch
        list_hashs_temp = [self.hsh[k] for k in indexes]#liste de hash du batch
        X, y = data_generation(list_IDs_temp, list_hashs_temp, self.static_df, self.max_values, self.padding)#récupération des observations et cibles du batch
        y=y.reshape(self.batch_size,5)
        return X.reshape(self.batch_size,*np.shape(X[0])), y, self.get_weight(y)#Génération du batch (observation,labels,poids)

    def get_weight(self, y):#Génération du poid pour une liste de cible
        w0=0.67569546#Ces valeurs ont été déterminé experimentalement
        w1=7.2109375
        w2=7.2109375
        w3=8.24107143
        w4=8.24107143
        weights=np.asarray([w0, w1, w2, w3, w4])
        return np.dot(y, weights)
    
    def on_epoch_end(self):
        self.indexes = np.arange(len(self.list_IDs))
        if self.shuffle == True:#mélange des indexe
            np.random.shuffle(self.indexes)
