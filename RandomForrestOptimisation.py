#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 14:12:13 2020

@author: hallavar
"""

import json
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, truncnorm, randint
from common import load_firstlayer_data, get_metrics
from Graphics import make_multiple_graph


def max_depth_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           min_sample_split=2,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=np.asarray(range(0,len(train_samples.columns)-1, 300))
        values[0]=2
    index=np.asarray(values)
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    for depth in values:
        rf_depth=RandomForestClassifier(max_depth=depth,
                                        min_samples_split=min_sample_split,
                                        max_leaf_nodes=max_leaf_nodes,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du max_depth : Entrainement pour max_depth = ',depth)        
        rf_depth.fit(train_samples, train_labels)
        predictions_depth= rf_depth.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_depth, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de max_depth')
            index=np.delete(index,np.where(index==depth))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour max_depth = {depth} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_depth_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_depth={X[i]}')
    
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('max_depth')
        ax.set_title(metric+' in fonction of max_depth')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of max_depth'
        return title, [X, Y], X[i]
    return X[i]

def min_sample_split_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=np.asarray([*range(5,len(train_samples)//2, 5)])
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=np.asarray(values)
    for min_sample in values:
        rf_min_samples_split=RandomForestClassifier(min_samples_split=min_sample,
                                        max_depth=max_depth,
                                        max_leaf_nodes=max_leaf_nodes,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du min_samples_split : Entrainement pour mins_sample_split = ',min_sample)
        rf_min_samples_split.fit(train_samples, train_labels)
        predictions_min_samples_split= rf_min_samples_split.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_min_samples_split, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de min_samples_split')
            index=np.delete(index,np.where(index==min_sample))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour min_samples_split = {min_sample} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('min_samples_split_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour min_samples_split={X[i]}')
    
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('min_sample_split')
        ax.set_title(metric+' in fonction of min_sample_split')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of min_sample_split'
        return title, [X, Y], X[i]
    return X[i]

def max_leaf_nodes_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=np.asarray(range(0,len(train_samples), 10))
        values[0]=2
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for max_leaf in values:
        rf_max_leaf_nodes=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        max_leaf_nodes=max_leaf,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du max_leaf_nodes : Entrainement pour max_leaf_nodes = ',max_leaf)
        rf_max_leaf_nodes.fit(train_samples, train_labels)
        predictions_max_leaf_nodes= rf_max_leaf_nodes.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_max_leaf_nodes, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de max_leaf_nodes')
            index=np.delete(index,np.where(index==max_leaf))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour max_leaf_nodes = {max_leaf} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_leaf_nodes_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_leaf_nodes={X[i]}')
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('max_leaf_nodes')
        ax.set_title(metric+' in fonction of max_leaf_nodes')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of max_leaf_nodes'
        return title, [X, Y], X[i]
    return X[i]

def min_samples_leaf_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           max_leaf_nodes=None,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values==None:
        values=np.asarray(range(0,len(train_samples)//70))
        values[0]=1
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for min_sample in values:
        rf_min_samples_leaf=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        min_samples_leaf=min_sample,
                                        max_leaf_nodes=max_leaf_nodes,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du min_samples_leaf : Entrainement pour min_samples_leaf = ',min_sample)
        rf_min_samples_leaf.fit(train_samples, train_labels)
        predictions_min_samples_leaf= rf_min_samples_leaf.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_min_samples_leaf, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de min_samples_leaf')
            index=np.delete(index,np.where(index==min_sample))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"min_samples_leaf = {min_sample} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('min_samples_leaf_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour min_samples_leaf={X[i]}')
    
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('min_samples_leaf')
        ax.set_title(metric+' in fonction of min_samples_leaf')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of min_samples_leaf'
        return title, [X, Y], X[i]
    return X[i]

def n_estimators_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           max_features='auto',
                           ):
    if values==None:
        values = np.asarray([*range(1,500,8)])
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for n_estimator in values:
        rf_n_estimators=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        min_samples_leaf=min_samples_leaf,
                                        max_leaf_nodes=max_leaf_nodes,
                                        n_estimators=n_estimator,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du n_estimators : Entrainement pour n_estimators = ',n_estimator)
        rf_n_estimators.fit(train_samples, train_labels)
        predictions_n_estimators= rf_n_estimators.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_n_estimators, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de n_estimators')
            index=np.delete(index,np.where(index==n_estimator))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"n_estimators = {n_estimator} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('n_estimators_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour n_estimators={X[i]}')
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('n_estimators')
        ax.set_title(metric+' in fonction of n_estimators')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of n_estimators'
        return title, [X, Y], X[i]
    return X[i]

def max_sample_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           max_leaf_nodes=None,
                           n_estimators=100,
                           min_samples_leaf=1,
                           max_features='auto',
                           ):
    if values==None:
        values = np.asarray([*range(2,len(train_samples)), None])
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for max_sample in values:
        rf_max_sample=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        min_samples_leaf=min_samples_leaf,
                                        max_leaf_nodes=max_leaf_nodes,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        max_sample=max_sample
                                        )
        if verbose:
            print('Optimisation du max_sample : Entrainement pour max_sample = ',max_sample)
        rf_max_sample.fit(train_samples, train_labels)
        predictions_max_sample= rf_max_sample.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_max_sample, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de n_estimators')
            index=np.delete(index,np.where(index==max_sample))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"max_sample = {max_sample} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_sample_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_sample={X[i]}')
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('max_sample')
        ax.set_title(metric+' in fonction of max_sample')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of max_sample'
        return title, [X, Y], X[i]
    return X[i]

def max_features_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           n_estimators=100,
                           ):
    if values==None:
        values = np.asarray([2,*range(100, len(train_samples.columns),150),None])
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for max_feature in values:
        rf_max_features=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        min_samples_leaf=min_samples_leaf,
                                        max_leaf_nodes=max_leaf_nodes,
                                        n_estimators=n_estimators,
                                        max_features=max_feature
                                        )
        if verbose:
            print('Optimisation du max_features : Entrainement pour max_features = ',max_feature)
        rf_max_features.fit(train_samples, train_labels)
        predictions_max_features= rf_max_features.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_max_features, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de max_features')
            index=np.delete(index,np.where(index==max_feature))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"max_features = {max_feature} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_feature_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_features={X[i]}')
    if graph:
        fig, ax=plt.subplots(1)
        ax.plot(X,Y, 'r--')
        ax.set_ylabel(metric)
        ax.set_xlabel('max_features')
        ax.set_title(metric+' in fonction of max_features')
        if i<len(X)-1:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        else:
            ax.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i-1], Y[i-1]),
                        arrowprops=dict(facecolor='black', shrink=0.05),
                        )
        fig.show
    else:
        title=metric+' in fonction of max_features'
        return title, [X, Y], X[i]
    return X[i]

def global_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=1, save=True, metric='f1', n_jobs=-1):
   
    verbose2=False
    if verbose==2:
        verbose2=True
    title, figure=[],[]
    
    tit, f, n_opti = n_estimators_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric) 
    if verbose>0:
        print("Fin de l'optimisation de n_estimators, valeur optimale : ",n_opti)
    figure.append(f)
    title.append(tit)
    tit, f, max_depth_opti=max_depth_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric, n_estimators=n_opti)
    if verbose>0:
        print("Fin de l'optimisation de max_depth, valeur optimale : ",max_depth_opti)
    figure.append(f)
    title.append(tit)
    tit, f, min_sample_split_opti=min_sample_split_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric, n_estimators=n_opti, max_depth=max_depth_opti)
    if verbose>0:
        print("Fin de l'optimisation de min_sample_split, valeur optimale : ",min_sample_split_opti)
    figure.append(f)
    title.append(tit)
    tit, f, max_leaf_nodes_opti=max_leaf_nodes_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric, n_estimators=n_opti, max_depth=max_depth_opti, min_samples_split=min_sample_split_opti)
    if verbose>0:
        print("Fin de l'optimisation de max_leaf_nodes, valeur optimale : ",max_leaf_nodes_opti)
    figure.append(f)
    title.append(tit)
    tit, f, min_samples_leaf_opti=min_samples_leaf_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric, n_estimators=n_opti, max_depth=max_depth_opti, min_samples_split=min_sample_split_opti,max_leaf_nodes=max_leaf_nodes_opti)
    if verbose>0:
        print("Fin de l'optimisation de min_sample_leaf, valeur optimale : ",min_samples_leaf_opti)
    figure.append(f)
    title.append(tit)
    tit, f, max_features_opti=max_features_optimisation(train_samples, train_labels, test_samples, test_labels, verbose=verbose2, graph=False, save=save, metric=metric, n_estimators=n_opti, max_depth=max_depth_opti, min_samples_split=min_sample_split_opti,max_leaf_nodes=max_leaf_nodes_opti, min_samples_leaf=min_samples_leaf_opti)
    if verbose>0:
        print("Fin de l'optimisation de max_features, valeur optimale : ",max_features_opti)
    figure.append(f)
    title.append(tit)
    
    graphs=dict(zip(title,figure))

    Title=f'Global Optimisation of {metric}'
    
    make_multiple_graph(graphs, Title, subtitle=True, arrow=True, figsize=(20, 10))
    
    params={'max_depth'          : max_depth_opti,
              'min_samples_split' : min_sample_split_opti,
              'max_leaf_nodes'    : max_leaf_nodes_opti,
              'min_samples_leaf'  : min_samples_leaf_opti,
              'n_estimators'      : n_opti,
              'max_features'      : max_features_opti,
            }
    
    if verbose>0:
        print("Entrainement de la Random Forest optimale")
    
    rf_opti=RandomForestClassifier(**params, n_jobs=n_jobs)         
    rf_opti.fit(train_samples, train_labels)
    
    if save:
        joblib.dump(rf_opti, "./RFOpti.joblib")
        print("Modèle sauvegardé")
        with open("best_params.json", "w") as fp:
            json.dump(params,fp,indent=1) 
    
    if verbose>0:
        print("Les meilleures parametres sont : ",params)
    
    return rf_opti
    
if __name__ == "__main__":
    Istlayer_dataset_path = "../Datasets/API-Intent-Traffic/Other_CSVs/1st_Layer/"

    train_samples, train_labels=load_firstlayer_data(Istlayer_dataset_path, data='Training')
    test_samples, test_labels=load_firstlayer_data(Istlayer_dataset_path, data='Testing')
    
    # fig, max_depth_opti=max_depth_optimisation(train_samples,train_labels,test_samples,test_labels, verbose=True, n_estimators=282, min_samples_leaf=2, graph=False, values=[10,20,30,40,50,100])
#    distrib={'max_depth'        : [1,*range(100,len(train_samples.columns)-1, 100),None],
#             'min_samples_split' : [*range(5,len(train_samples)//2, 5)],
#             'max_leaf_nodes'   : [2,*range(10,len(train_samples), 8),None],
#             'min_samples_leaf' : [None,*range(1,len(train_samples)//100)],
#             'n_estimators'     : [*range(1,500)],
#             'max_sample'       : [None, *range(5,len(train_samples)//2, 5)],
#             'max_features'     : [1,*range(100,len(train_samples.columns)-1, 100),None],
#             'criterion'        : ["gini", "entropy"]
#            }
    distrib={'max_depth'        : [*range(1,1000,10),None],
             'min_samples_split' : [*range(5,len(train_samples)//2, 5)],
#             'max_leaf_nodes'   : [*range(2, 100, 5), None],
#             'min_samples_leaf' : [*range(1, 100, 5), None],
             'n_estimators'     : [*range(1,500)],
#             'max_sample'       : [*range(1, 100, 5), None],
             'max_features'     : ['auto', 'sqrt', 'log2'],
             'criterion'        : ["gini", "entropy"]
            }
#    rf = RandomForestClassifier()
#    rf_opti = RandomizedSearchCV(rf, distrib, cv=5, n_iter=1000, random_state=2, scoring='f1', verbose=3, n_jobs=-1, error_score=0.0)
#    
#    rf_opti.fit(train_samples, train_labels)
#    
#    print(f"Les meilleurs parametres sont {rf_opti.best_params_} ")
#    print(f"Le meilleur score est f1 : {rf_opti.best_score_}")

    rf=global_optimisation(train_samples,train_labels,test_samples,test_labels, verbose=2)