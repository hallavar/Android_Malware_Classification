#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Nov 22 14:12:13 2020

@author: hallavar
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, truncnorm, randint
from common import load_firstlayer_data, get_metrics


def max_depth_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           min_sample_split=2,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=np.asarray(range(0,len(train_samples.columns)-1, 100))
        values[0]=1
    index=values
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    for depth in values:
        rf_depth=RandomForestClassifier(max_depth=depth,
                                        min_samples_split=min_sample_split,
                                        max_leaf_nodes=max_leaf_nodes,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du max_depth : Entrainement pour max_depth = ',depth)        
        rf_depth.fit(train_samples, train_labels)
        predictions_depth= rf_depth.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_depth, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de max_depth')
            index=np.delete(index,np.where(index==depth))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour max_depth = {depth} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_depth_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if graph:
        plt.plot(X,Y, 'r--')
        plt.ylabel(metric)
        plt.xlabel('max_depth')
        plt.title(metric+' in fonction of max_depth')
        plt.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )
        plt.show
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_depth={X[i]}')
    return X[i]

def min_sample_split_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           max_leaf_nodes=None,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=[*range(5,len(train_samples)//2, 5)]
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=np.asarray(values)
    for min_sample in values:
        rf_min_samples_split=RandomForestClassifier(min_samples_split=min_sample,
                                        max_depth=max_depth,
                                        max_leaf_nodes=max_leaf_nodes,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du min_samples_split : Entrainement pour mins_sample_split = ',min_sample)
        rf_min_samples_split.fit(train_samples, train_labels)
        predictions_min_samples_split= rf_min_samples_split.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_min_samples_split, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de min_samples_split')
            index=np.delete(index,np.where(index==min_sample))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour min_samples_split = {min_sample} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('min_samples_split_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if graph:
        plt.plot(X,Y, 'b--')
        plt.ylabel(metric)
        plt.xlabel('min_samples_split')
        plt.title(metric+' in fonction of min_samples_split')
        plt.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )
        plt.show
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour min_samples_split={X[i]}')
    return X[i]

def max_leaf_nodes_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           min_samples_leaf=1,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values == None:
        values=np.asarray(range(0,len(train_samples), 8))
        values[0]=2
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for max_leaf in values:
        rf_max_leaf_nodes=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        max_leaf_nodes=max_leaf,
                                        min_samples_leaf=min_samples_leaf,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du max_leaf_nodes : Entrainement pour max_leaf_nodes = ',max_leaf)
        rf_max_leaf_nodes.fit(train_samples, train_labels)
        predictions_max_leaf_nodes= rf_max_leaf_nodes.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_max_leaf_nodes, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de max_leaf_nodes')
            index=np.delete(index,np.where(index==max_leaf))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"Pour max_leaf_nodes = {max_leaf} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('max_leaf_nodes_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if graph:
        plt.plot(X,Y, 'g--')
        plt.ylabel(metric)
        plt.xlabel('max_leaf_nodes')
        plt.title(metric+' in fonction of max_leaf_nodes')
        plt.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )
        plt.show
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour max_leaf_nodes={X[i]}')
    return X[i]

def min_samples_leaf_optimisation(train_samples,
                           train_labels,
                           test_samples,
                           test_labels,
                           values = None,
                           verbose=False,
                           graph=True,
                           save=True,
                           metric='f1',
                           max_depth=1000,
                           min_samples_split=2,
                           max_leaf_nodes=None,
                           n_estimators=100,
                           max_features='auto',
                           ):
    if values==None:
        values=np.asarray(range(0,len(train_samples)//100))
        values[0]=1
    metrics=pd.DataFrame(columns=['accuracy','precision','recall','f1'])
    index=values
    for min_sample in values:
        rf_min_samples_leaf=RandomForestClassifier(min_samples_split=min_samples_split,
                                        max_depth=max_depth,
                                        min_samples_leaf=min_sample,
                                        max_leaf_nodes=max_leaf_nodes,
                                        n_estimators=n_estimators,
                                        max_features=max_features,
                                        )
        if verbose:
            print('Optimisation du min_samples_leaf : Entrainement pour min_samples_leaf = ',min_sample)
        rf_min_samples_leaf.fit(train_samples, train_labels)
        predictions_min_samples_leaf= rf_min_samples_leaf.predict(test_samples)
        arr=np.asarray(get_metrics(test_labels, predictions_min_samples_leaf, get_cm=False))
        if np.isnan(arr).any():
            if verbose:
                print('Il y a eu une erreur pour cette valeur de min_samples_leaf')
            index=np.delete(index,np.where(index==min_sample))
        else:
            metrics=metrics.append(pd.DataFrame(arr.reshape(1,-1), columns=list(metrics)), ignore_index=True)
            if verbose:
                print(f"min_samples_leaf = {min_sample} ; "+metric+f" = {metrics.at[len(metrics)-1, metric]}")
    metrics=metrics.set_index(index)
    if save:
        metrics.to_csv('min_samples_leaf_optimisation.csv', index=True)
        if verbose :
            print("Résultats intermédiaires de l'optimisation sauvegardés")
    X=np.asarray(metrics.index)
    Y=metrics[metric].to_numpy()
    i=np.argmax(Y)
    if graph:
        plt.plot(X,Y, 'g+')
        plt.ylabel(metric)
        plt.xlabel('min_samples_leaf')
        plt.title(metric+' in fonction of min_samples_leaf')
        plt.annotate('local max', xy=(X[i], Y[i]), xytext=(X[i+1], Y[i+1]),
             arrowprops=dict(facecolor='black', shrink=0.05),
             )
        plt.show
    if verbose :
        print(f'Le maximum pour {metric} est de {Y[i]} et est atteint pour min_samples_leaf={X[i]}')
    return X[i]



if __name__ == "__main__":
    Istlayer_dataset_path = "../Datasets/API-Intent-Traffic/Other_CSVs/1st_Layer/"

    train_samples, train_labels=load_firstlayer_data(Istlayer_dataset_path, data='Training')
    test_samples, test_labels=load_firstlayer_data(Istlayer_dataset_path, data='Testing')
    
#    max_depth_opti=min_sample_split_optimisation(train_samples,train_labels,test_samples,test_labels, verbose=True, n_estimators=282, min_samples_leaf=2)
#    distrib={'max_depth'        : [1,*range(100,len(train_samples.columns)-1, 100),None],
#             'min_samples_split' : [*range(5,len(train_samples)//2, 5)],
#             'max_leaf_nodes'   : [2,*range(10,len(train_samples), 8),None],
#             'min_samples_leaf' : [None,*range(1,len(train_samples)//100)],
#             'n_estimators'     : [*range(1,500)],
#             'max_sample'       : [None, *range(5,len(train_samples)//2, 5)],
#             'max_features'     : [1,*range(100,len(train_samples.columns)-1, 100),None],
#             'criterion'        : ["gini", "entropy"]
#            }
    distrib={'max_depth'        : [10,20,30,40,50,100,None],
             'min_samples_split' : [2, 5, 10, 15, 20],
#             'max_leaf_nodes'   : [*range(2, 100, 5), None],
#             'min_samples_leaf' : [*range(1, 100, 5), None],
             'n_estimators'     : [*range(1,500)],
#             'max_sample'       : [*range(1, 100, 5), None],
             'max_features'     : ['auto', 'sqrt', 'log2'],
             'criterion'        : ["gini", "entropy"]
            }
    rf = RandomForestClassifier()
    rf_opti = RandomizedSearchCV(rf, distrib, cv=5, n_iter=200, random_state=2, scoring='f1')
    
    rf_opti.fit(train_samples, train_labels)
    
    print(f"Les meilleurs parametres sont {rf_opti.best_params_} ")
    print(f"Le meilleur score est f1 : {rf_opti.best_score_}")
    